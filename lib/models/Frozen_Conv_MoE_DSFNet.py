import torch
from torch import nn
import torch.nn.functional as F
import os

# å¯¼å…¥åŸå§‹çš„DSFNetå’Œload_modelå‡½æ•°
from .DSFNet import DSFNet as DSFNet_expert
from .stNet import load_model


class ConvGatingNetwork(nn.Module):
    """
    ä¸€ä¸ªæ–°çš„ã€æ›´å¼ºå¤§çš„é—¨æ§ç½‘ç»œã€‚
    ä½¿ç”¨ä¸€ä¸ª64é€šé“çš„å·ç§¯å±‚ä½œä¸ºä¸»å¹²ï¼Œåé¢æ¥FCå±‚ã€‚
    è¾“å…¥ï¼š[B, C, T, H, W]
    è¾“å‡ºï¼šç¨€ç–æƒé‡ [B, num_experts] (åªæœ‰ä¸€ä¸ªå…ƒç´ ä¸º1ï¼Œå…¶ä½™ä¸º0)
    """

    def __init__(self, num_experts, top_k=1):
        super(ConvGatingNetwork, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k

        # ä¸»å¹²ç½‘ç»œ: Conv -> BN -> ReLU -> MaxPool
        # è¾“å…¥é€šé“ä¸º3(RGB), æ—¶é—´ç»´åº¦ä¸º5
        # æˆ‘ä»¬ä½¿ç”¨2Då·ç§¯å¤„ç†æ—¶é—´åºåˆ—ï¼Œå°†æ—¶é—´ç»´åº¦è§†ä¸ºé€šé“çš„ä¸€éƒ¨åˆ†
        # (B, C, T, H, W) -> (B, C*T, H, W)
        self.backbone = nn.Sequential(
            nn.Conv2d(3 * 5, 64, kernel_size=7, stride=2, padding=3, bias=False),  # in_channels = 3 * 5 = 15
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # H/4, W/4
        )

        # åˆ†ç±»å¤´
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(64, self.num_experts)

    def forward(self, x):
        b, c, t, h, w = x.shape
        # å°†æ—¶é—´å’Œé€šé“ç»´åº¦åˆå¹¶ï¼Œä»¥é€‚åº”2Då·ç§¯
        x_reshaped = x.view(b, c * t, h, w)

        features = self.backbone(x_reshaped)
        features = self.avg_pool(features)
        features = features.view(b, -1)

        # logits: [B, num_experts], è¿™æ˜¯æ¯ä¸ªä¸“å®¶çš„åˆ†æ•°
        logits = self.fc(features)

        # ==================== [æ ¸å¿ƒä¿®æ”¹: è¾“å‡º0/1] ====================
        # é€‰å‡º Top-K çš„ä¸“å®¶ç´¢å¼•
        _, top_k_indices = torch.topk(logits, self.top_k, dim=1)

        # ç”Ÿæˆä¸€ä¸ªå…¨é›¶çš„one-hotç¼–ç å¼ é‡
        sparse_gates_one_hot = torch.zeros_like(logits)
        # å°†è¢«é€‰ä¸­ä¸“å®¶çš„ä½ç½®è®¾ç½®ä¸º1
        sparse_gates_one_hot.scatter_(1, top_k_indices, 1)
        # ==========================================================

        # è¾…åŠ©æŸå¤± (Load Balancing Loss) - ä¿æŒä¸å˜
        expert_mask = sparse_gates_one_hot
        density = expert_mask.mean(dim=0)
        density_proxy = F.softmax(logits, dim=1).mean(dim=0)
        aux_loss = (density * density_proxy).sum() * self.num_experts

        # è¿”å› one-hot ç¼–ç çš„ç¨€ç–é—¨æ§ã€ä¸“å®¶ç´¢å¼•å’Œè¾…åŠ©æŸå¤±
        return sparse_gates_one_hot, top_k_indices, aux_loss


class FrozenConvMoE_DSFNet(nn.Module):
    """
    ä½¿ç”¨æ–°çš„å·ç§¯é—¨æ§ç½‘ç»œï¼Œå¹¶å†»ç»“ä¸“å®¶ã€‚
    """

    def __init__(self, heads, head_conv=128, num_experts=3, top_k=1, loss_coef=1e-2, pretrained_paths=None):
        super(FrozenConvMoE_DSFNet, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.loss_coef = loss_coef
        self.heads = heads

        print("ğŸš€ åˆå§‹åŒ– Frozen-Conv-MoE-DSFNet æ¨¡å‹")
        print(f"   - é—¨æ§ç½‘ç»œ: Conv-64F Backbone")
        print(f"   - ä¸“å®¶æ€»æ•°: {self.num_experts}")
        print(f"   - æ¿€æ´»ä¸“å®¶æ•° (Top-K): {self.top_k} (ç¡¬é€‰æ‹©)")

        # 1. å®ä¾‹åŒ–æ–°çš„å·ç§¯é—¨æ§ç½‘ç»œ
        self.gating_network = ConvGatingNetwork(self.num_experts, self.top_k)

        # 2. å®ä¾‹åŒ–ã€åŠ è½½å¹¶å†»ç»“ä¸“å®¶ (é€»è¾‘ä¸ä¹‹å‰ç›¸åŒ)
        self.experts = nn.ModuleList()
        if pretrained_paths is None or len(pretrained_paths) != self.num_experts:
            raise ValueError(f"å¿…é¡»æä¾›ä¸€ä¸ªåŒ…å« {self.num_experts} ä¸ªé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„çš„åˆ—è¡¨ã€‚")

        for i, path in enumerate(pretrained_paths):
            print(f"   - åŠ è½½å¹¶å†»ç»“ä¸“å®¶ {i + 1}/{self.num_experts} ä»: '{os.path.basename(path)}'")
            expert_model = DSFNet_expert(heads, head_conv)

            if os.path.exists(path):
                expert_model = load_model(expert_model, path)
            else:
                print(f"   - âš ï¸ è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ {path}ã€‚")

            expert_model.eval()
            for param in expert_model.parameters():
                param.requires_grad = False

            self.experts.append(expert_model)

        print("âœ… æ‰€æœ‰ä¸“å®¶å·²åŠ è½½å¹¶å†»ç»“ã€‚")

    def forward(self, x):
        batch_size = x.shape[0]

        # 1. è·å–é—¨æ§è¾“å‡º (0/1ç¼–ç )
        sparse_gates_one_hot, top_k_indices, aux_loss = self.gating_network(x)

        # 2. åˆå§‹åŒ–æœ€ç»ˆè¾“å‡º
        final_outputs = {head: torch.zeros(batch_size, *self.experts[0](x)[0][head].shape[1:], device=x.device) for head
                         in self.heads}

        # 3. éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
        for i in range(batch_size):
            # è·å–è¢«é€‰ä¸­çš„ä¸“å®¶ç´¢å¼• (å› ä¸ºtop_k=1, è¿™é‡Œåªæœ‰ä¸€ä¸ªç´¢å¼•)
            expert_idx = top_k_indices[i][0].item()
            sample_input = x[i].unsqueeze(0)

            # 4. åªè¿è¡Œè¢«é€‰ä¸­çš„é‚£ä¸ªä¸“å®¶
            with torch.no_grad():
                expert_output_dict = self.experts[expert_idx](sample_input)[0]

            # 5. ç›´æ¥å°†ä¸“å®¶è¾“å‡ºä½œä¸ºè¯¥æ ·æœ¬çš„æœ€ç»ˆè¾“å‡º
            for head_name, head_tensor in expert_output_dict.items():
                final_outputs[head_name][i] = head_tensor.squeeze(0)

        return [final_outputs], self.loss_coef * aux_loss