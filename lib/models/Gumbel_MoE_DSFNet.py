import torch
from torch import nn
import torch.nn.functional as F
import os

# å¯¼å…¥åŸå§‹çš„DSFNetä½œä¸ºæˆ‘ä»¬çš„â€œä¸“å®¶â€
from .DSFNet import DSFNet as DSFNet_expert, fill_fc_weights
# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„ load_model å‡½æ•°
from .stNet import load_model


class GumbelGatingNetwork(nn.Module):
    """
    ä½¿ç”¨Gumbel-Softmaxçš„é—¨æ§ç½‘ç»œï¼Œæ”¯æŒTop-Kä¸“å®¶é€‰æ‹©ã€‚
    - è®­ç»ƒæ—¶ä½¿ç”¨ Gumbel-Softmax å®ç°å¯¹Top-Kä¸ªä¸“å®¶çš„å¯å¯¼é€‰æ‹©ã€‚
    - è¯„ä¼°æ—¶ä½¿ç”¨ ArgMax å®ç°å¯¹Top-Kä¸ªä¸“å®¶çš„ç¡®å®šæ€§é€‰æ‹©ã€‚
    """

    def __init__(self, num_experts, top_k=1):
        """
        åˆå§‹åŒ–é—¨æ§ç½‘ç»œã€‚
        Args:
            num_experts (int): ä¸“å®¶æ€»æ•°ã€‚
            top_k (int): æ¯ä¸ªè¾“å…¥éœ€è¦æ¿€æ´»çš„ä¸“å®¶æ•°é‡ã€‚
        """
        super(GumbelGatingNetwork, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k

        # ä¸€ä¸ªè½»é‡çº§çš„CNNï¼Œç”¨äºä»è¾“å…¥ä¸­æå–ç‰¹å¾ä»¥å†³å®šä¸“å®¶æƒé‡
        self.backbone = nn.Sequential(
            # è¾“å…¥å½¢çŠ¶: [B, C*T, H, W], ä¾‹å¦‚ [B, 15, H, W]
            nn.Conv2d(3 * 5, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸ H/4, W/4
        )
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(64, self.num_experts)

    def forward(self, x):
        b, c, t, h, w = x.shape
        # å°†æ—¶é—´å’Œé€šé“ç»´åº¦åˆå¹¶ï¼Œä»¥é€‚åº”2Då·ç§¯: [B, C, T, H, W] -> [B, C*T, H, W]
        x_reshaped = x.view(b, c * t, h, w)

        features = self.backbone(x_reshaped)
        features = self.avg_pool(features)
        features = features.view(b, -1)

        # logits: [B, num_experts], æ¯ä¸ªä¸“å®¶çš„åŸå§‹åˆ†æ•°
        logits = self.fc(features)

        # é€‰å‡º Top-K çš„ä¸“å®¶ç´¢å¼•å’Œå¯¹åº”çš„åˆ†æ•°(logits)
        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=1)

        # åˆ›å»ºä¸€ä¸ªç”¨äºå­˜å‚¨é—¨æ§æƒé‡çš„ç¨€ç–å¼ é‡
        sparse_gates = torch.zeros_like(logits)

        if self.training:
            # è®­ç»ƒæ—¶: åœ¨ top-k çš„ logits ä¸Šåº”ç”¨ Gumbel-Softmax
            # hard=True ä¼šåœ¨å‰å‘ä¼ æ’­æ—¶äº§ç”Ÿ one-hot è¾“å‡ºï¼Œä½†åœ¨åå‘ä¼ æ’­æ—¶ä½¿ç”¨ softmax çš„æ¢¯åº¦
            gumbel_top_k = F.gumbel_softmax(top_k_logits, tau=1.0, hard=True, dim=-1)

            # å°† Gumbel-Softmax çš„ç»“æœï¼ˆè¿‘ä¼¼0/1ï¼‰æ”¾å›ç¨€ç–å¼ é‡çš„æ­£ç¡®ä½ç½®
            sparse_gates.scatter_(1, top_k_indices, gumbel_top_k)
        else:
            # è¯„ä¼°æ—¶: ç›´æ¥å°†è¢«é€‰ä¸­çš„ä¸“å®¶ä½ç½®èµ‹äºˆæƒé‡
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ 1.0 / self.top_kï¼Œå®ç°å¯¹ top-k ä¸ªä¸“å®¶çš„ç­‰æƒé‡å¹³å‡é›†æˆ
            sparse_gates.scatter_(1, top_k_indices, 1.0 / self.top_k)

        # è¾…åŠ©æŸå¤± (Load Balancing Loss)ï¼Œé¼“åŠ±é—¨æ§ç½‘ç»œå‡åŒ€åœ°ä½¿ç”¨æ‰€æœ‰ä¸“å®¶
        # 1. åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„0/1æ©ç æ¥è¡¨ç¤ºå“ªäº›ä¸“å®¶è¢«é€‰ä¸­
        expert_mask = torch.zeros_like(logits).scatter_(1, top_k_indices, 1)
        # 2. è®¡ç®—æ¯ä¸ªä¸“å®¶åœ¨æ‰¹æ¬¡ä¸­è¢«é€‰ä¸­çš„é¢‘ç‡
        density = expert_mask.mean(dim=0)
        # 3. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡è·¯ç”±æ¦‚ç‡
        density_proxy = F.softmax(logits, dim=1).mean(dim=0)
        # 4. æŸå¤±æ˜¯è¿™ä¸¤è€…çš„ç‚¹ç§¯ä¹‹å’Œï¼Œå†ä¹˜ä»¥ä¸“å®¶æ•°é‡ä»¥ä¿æŒå°ºåº¦
        aux_loss = (density * density_proxy).sum() * self.num_experts

        return sparse_gates, top_k_indices, aux_loss


class Gumbel_MoE_DSFNet(nn.Module):
    """
    ä½¿ç”¨Gumbel-Softmaxé—¨æ§è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„MoEæ¨¡å‹ã€‚
    æ”¯æŒé€‰æ‹© Top-K ä¸ªä¸“å®¶ã€‚
    """

    def __init__(self, heads, head_conv=128, num_experts=3, top_k=1, loss_coef=1e-2,
                 pretrained_paths=None, expert_modules=None):
        super(Gumbel_MoE_DSFNet, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.loss_coef = loss_coef
        self.heads = heads

        print(f"ğŸš€ åˆå§‹åŒ– Gumbel-MoE-DSFNet æ¨¡å‹ (Top-K={self.top_k})")
        print(f"   - é—¨æ§æœºåˆ¶: è®­ç»ƒæ—¶ Top-K Gumbel-Softmax, è¯„ä¼°æ—¶ Top-K ArgMax")
        print(f"   - ä¸“å®¶æ€»æ•°: {self.num_experts}")

        # å°† top_k å‚æ•°ä¼ é€’ç»™é—¨æ§ç½‘ç»œ
        self.gating_network = GumbelGatingNetwork(self.num_experts, self.top_k)

        if expert_modules is not None:
            print("   - ä½¿ç”¨å¤–éƒ¨æä¾›çš„ä¸“å®¶æ¨¡å—åˆ—è¡¨ã€‚")
            if len(expert_modules) != self.num_experts:
                raise ValueError(f"æä¾›çš„ä¸“å®¶æ¨¡å—æ•°é‡({len(expert_modules)})ä¸num_experts({self.num_experts})ä¸åŒ¹é…ã€‚")
            self.experts = expert_modules
        elif pretrained_paths is not None:
            print("   - æ ¹æ®è·¯å¾„åˆ—è¡¨åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸“å®¶ã€‚")
            if len(pretrained_paths) * (1 + 4) != self.num_experts and len(
                    pretrained_paths) != self.num_experts:  # å…¼å®¹æ—§çš„åˆå§‹åŒ–é€»è¾‘
                raise ValueError(f"æä¾›çš„è·¯å¾„æ•°é‡({len(pretrained_paths)})ä¸ä¸“å®¶æ•°é‡({self.num_experts})é…ç½®ä¸åŒ¹é…ã€‚")

            self.experts = nn.ModuleList()
            for i, path in enumerate(pretrained_paths):
                print(f"   - åˆå§‹åŒ–ä¸“å®¶ {i + 1}/{len(pretrained_paths)} (åŠå‰¯æœ¬) ä»: '{os.path.basename(path)}'")
                expert_model = DSFNet_expert(heads, head_conv)
                if os.path.exists(path):
                    expert_model = load_model(expert_model, path)
                else:
                    print(f"   - âš ï¸ è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ {path}ã€‚ä¸“å®¶å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ã€‚")

                # æ ¹æ® expert_list çš„é•¿åº¦æ¥å†³å®šæ˜¯å¦è¦å¤åˆ¶ä¸“å®¶
                if len(self.experts) < self.num_experts:
                    self.experts.append(expert_model)

                # å¦‚æœéœ€è¦å¤åˆ¶ä¸“å®¶ä»¥è¾¾åˆ° num_experts
                replicas_to_add = (self.num_experts // len(pretrained_paths)) - 1
                for _ in range(replicas_to_add):
                    if len(self.experts) < self.num_experts:
                        perturbed_expert = deepcopy(expert_model)
                        # è¿™é‡Œå¯ä»¥æ·»åŠ æ‰°åŠ¨é€»è¾‘ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å…ˆç›´æ¥å¤åˆ¶
                        self.experts.append(perturbed_expert)

        else:
            raise ValueError("å¿…é¡»æä¾› 'pretrained_paths' æˆ– 'expert_modules'ã€‚")

        print(f"âœ… æ‰€æœ‰ {len(self.experts)} ä¸ªä¸“å®¶æ¨¡å—å·²è®¾ç½®ã€‚")

    def forward(self, x):
        # 1. ä»é—¨æ§ç½‘ç»œè·å–ç¨€ç–æƒé‡
        # sparse_gates å½¢çŠ¶: [batch_size, num_experts]
        # å€¼ç¤ºä¾‹ (è®­ç»ƒ, k=2): [0, 1, 0, 0, 1, 0, ...]
        # å€¼ç¤ºä¾‹ (æµ‹è¯•, k=2): [0, 0.5, 0, 0, 0.5, 0, ...]
        sparse_gates, top_k_indices, aux_loss = self.gating_network(x)

        # 2. è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„è¾“å‡º
        # è¿™æ˜¯ä¸ºäº†ç¡®ä¿æ¢¯åº¦èƒ½å¤Ÿæµå‘æ‰€æœ‰ä¸“å®¶ï¼Œå³ä½¿å®ƒä»¬å½“å‰æœªè¢«æ¿€æ´»
        expert_outputs = [expert(x)[0] for expert in self.experts]

        # 3. åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æœ€ç»ˆçš„èšåˆè¾“å‡º
        final_outputs = {head: 0.0 for head in self.heads}

        # 4. åŠ æƒæ±‚å’Œ
        for i in range(self.num_experts):
            # è·å–ç¬¬iä¸ªä¸“å®¶çš„é—¨æ§æƒé‡, å½¢çŠ¶: [batch_size]
            # å¹¶å°†å…¶ reshape ä»¥ä¾¿ä¸è¾“å‡ºå¼ é‡è¿›è¡Œå¹¿æ’­: [batch_size, 1, 1, 1]
            gate_reshaped = sparse_gates[:, i].view(-1, 1, 1, 1)

            # è·å–ç¬¬iä¸ªä¸“å®¶çš„è¾“å‡ºå­—å…¸
            expert_head_dict = expert_outputs[i]

            # éå†æ¯ä¸ªè¾“å‡ºå¤´ (e.g., 'hm', 'wh', 'reg')
            for head_name, head_tensor in expert_head_dict.items():
                # å°†è¯¥ä¸“å®¶çš„è¾“å‡ºä¹˜ä»¥å…¶æƒé‡ï¼Œå¹¶ç´¯åŠ åˆ°æœ€ç»ˆç»“æœä¸­
                # å¦‚æœä¸€ä¸ªä¸“å®¶æœªè¢«é€‰ä¸­ï¼Œå…¶ gate_reshaped çš„å€¼ä¸º0ï¼Œä¹˜ç§¯ä¹Ÿä¸º0
                final_outputs[head_name] += gate_reshaped * head_tensor

        # è¿”å›æœ€ç»ˆçš„é¢„æµ‹ç»“æœå’Œç”¨äºè®­ç»ƒçš„è¾…åŠ©æŸå¤±
        return [final_outputs], self.loss_coef * aux_loss