import torch
from torch import nn
import torch.nn.functional as F
import os

# å¯¼å…¥åŸå§‹çš„DSFNetä½œä¸ºæˆ‘ä»¬çš„â€œä¸“å®¶â€
from .DSFNet import DSFNet as DSFNet_expert


class SparseGatingNetwork(nn.Module):
    """
    ç¨€ç–é—¨æ§ç½‘ç»œï¼šä¸ºæ¯ä¸ªä¸“å®¶ç”Ÿæˆä¸€ä¸ªåˆ†æ•°ï¼ˆlogitï¼‰ï¼Œå¹¶é€‰æ‹©åˆ†æ•°æœ€é«˜çš„Top-Kä¸ªä¸“å®¶ã€‚
    """

    def __init__(self, num_experts, top_k):
        super(SparseGatingNetwork, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k

        # ä¸ä¹‹å‰ç›¸åŒï¼Œä¸€ä¸ªè½»é‡çº§çš„CNNæ¥æå–å…¨å±€ç‰¹å¾
        self.feature_extractor = nn.Sequential(
            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=1),
            nn.BatchNorm3d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),

            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
        )

        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Linear(32, self.num_experts)

    def forward(self, x):
        b = x.shape[0]
        features = self.feature_extractor(x)
        features = self.avg_pool(features)
        features = features.view(b, -1)

        # logits: [B, num_experts], è¿™æ˜¯æ¯ä¸ªä¸“å®¶çš„åˆ†æ•°
        logits = self.fc(features)

        # é€‰å‡º Top-K çš„ä¸“å®¶
        # top_k_gates: æƒé‡, top_k_indices: ç´¢å¼•
        top_k_gates, top_k_indices = torch.topk(logits, self.top_k, dim=1)

        # ç¨€ç–é—¨æ§ï¼šåˆ›å»ºä¸€ä¸ªå…¨é›¶å¼ é‡ï¼Œåªåœ¨è¢«é€‰ä¸­çš„ä¸“å®¶ä½ç½®ä¸Šå¡«å……æƒé‡
        # æˆ‘ä»¬ä½¿ç”¨softmaxæ¥å½’ä¸€åŒ–è¿™ top_k ä¸ªæƒé‡
        sparse_gates = torch.zeros_like(logits)
        sparse_gates.scatter_(1, top_k_indices, F.softmax(top_k_gates, dim=1))

        # è¾…åŠ©æŸå¤± (Load Balancing Loss)
        # è¿™ä¸€æ­¥å¯¹äºç¨€ç–MoEè‡³å…³é‡è¦ï¼Œå®ƒé¼“åŠ±é—¨æ§ç½‘ç»œä½¿ç”¨ä¸åŒçš„ä¸“å®¶
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
        expert_mask = torch.zeros_like(logits)
        expert_mask.scatter_(1, top_k_indices, 1)  # è¢«é€‰ä¸­çš„ä¸“å®¶ä½ç½®ä¸º1

        density = expert_mask.mean(dim=0)
        density_proxy = F.softmax(logits, dim=1).mean(dim=0)
        aux_loss = (density * density_proxy).sum() * self.num_experts

        return sparse_gates, top_k_indices, aux_loss


class SparseEnsembleMoE_DSFNet(nn.Module):
    """
    ç¨€ç–æ¨¡å‹é›†æˆå¼MoE DSFNet
    """

    def __init__(self, heads, head_conv=128, num_experts=15, top_k=3, loss_coef=1e-2, pretrained_paths=None):
        super(SparseEnsembleMoE_DSFNet, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.loss_coef = loss_coef
        self.heads = heads

        print(f"ğŸš€ åˆå§‹åŒ–ç¨€ç– Ensemble-MoE-DSFNet æ¨¡å‹")
        print(f"   - ä¸“å®¶æ€»æ•°: {self.num_experts}")
        print(f"   - æ¿€æ´»ä¸“å®¶æ•° (Top-K): {self.top_k}")

        # 1. å®ä¾‹åŒ–ç¨€ç–é—¨æ§ç½‘ç»œ
        self.gating_network = SparseGatingNetwork(self.num_experts, self.top_k)

        # 2. å®ä¾‹åŒ–å¹¶åŠ è½½é¢„è®­ç»ƒçš„DSFNetä¸“å®¶
        self.experts = nn.ModuleList()
        if pretrained_paths is None or len(pretrained_paths) != 3:
            raise ValueError("å¿…é¡»æä¾›ä¸€ä¸ªåŒ…å«3ä¸ªé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„çš„åˆ—è¡¨ `pretrained_paths`ã€‚")

        experts_per_pth = self.num_experts // len(pretrained_paths)
        print(f"   - æ¯ä¸ªé¢„è®­ç»ƒæ¨¡å‹å°†è¢«ç”¨äºåˆå§‹åŒ– {experts_per_pth} ä¸ªä¸“å®¶ã€‚")

        for i, path in enumerate(pretrained_paths):
            if not os.path.exists(path):
                print(f"âš ï¸ è­¦å‘Š: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {path}ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ä¸“å®¶ã€‚")
                state_dict = None
            else:
                print(f"   - æ­£åœ¨ä» '{os.path.basename(path)}' åŠ è½½æƒé‡...")
                checkpoint = torch.load(path, map_location='cpu')
                # å…¼å®¹ä¸åŒçš„checkpointæ ¼å¼
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint

            for j in range(experts_per_pth):
                expert_model = DSFNet_expert(heads, head_conv)
                if state_dict:
                    # æ¸…ç† 'module.' å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    clean_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
                    expert_model.load_state_dict(clean_state_dict, strict=False)

                self.experts.append(expert_model)

        print("âœ… æ‰€æœ‰ä¸“å®¶å·²æˆåŠŸåˆå§‹åŒ–ã€‚")

    def forward(self, x):
        batch_size = x.shape[0]

        # 1. é€šè¿‡é—¨æ§ç½‘ç»œè·å–Top-Kä¸“å®¶çš„æƒé‡å’Œç´¢å¼•
        sparse_gates, top_k_indices, aux_loss = self.gating_network(x)

        # 2. åˆå§‹åŒ–ç”¨äºå­˜å‚¨æ¿€æ´»ä¸“å®¶è¾“å‡ºçš„åˆ—è¡¨
        expert_outputs_list = []

        # 3. åªè®¡ç®—è¢«é€‰ä¸­çš„ä¸“å®¶çš„è¾“å‡º
        for i in range(batch_size):
            # è·å–å½“å‰æ ·æœ¬é€‰æ‹©çš„ä¸“å®¶ç´¢å¼•
            active_indices = top_k_indices[i]
            # è·å–å½“å‰æ ·æœ¬çš„è¾“å…¥
            sample_input = x[i].unsqueeze(0)

            # åˆ†åˆ«è®¡ç®—æ¯ä¸ªæ¿€æ´»ä¸“å®¶çš„è¾“å‡º
            active_outputs = [self.experts[idx](sample_input)[0] for idx in active_indices]
            expert_outputs_list.append(active_outputs)

        # 4. æ•´åˆç»“æœï¼šå¯¹æ¿€æ´»ä¸“å®¶çš„è¾“å‡ºè¿›è¡ŒåŠ æƒå¹³å‡
        final_outputs = {head: torch.zeros(batch_size, *expert_outputs_list[0][0][head].shape[1:], device=x.device) for
                         head in self.heads}

        for i in range(batch_size):
            active_indices = top_k_indices[i]
            active_gates = sparse_gates[i][active_indices]  # è·å–éé›¶çš„æƒé‡

            for k in range(self.top_k):
                expert_output_dict = expert_outputs_list[i][k]
                gate_val = active_gates[k]

                for head_name, head_tensor in expert_output_dict.items():
                    final_outputs[head_name][i] += gate_val * head_tensor.squeeze(0)

        final_result = [final_outputs]

        return final_result, self.loss_coef * aux_loss